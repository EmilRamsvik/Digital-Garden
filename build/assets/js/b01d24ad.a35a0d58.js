"use strict";(self.webpackChunkdigital_garden=self.webpackChunkdigital_garden||[]).push([[38916],{7227:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var i=n(85893),a=n(11151);const r={Finished:null,tags:["article","digital-garden"],created:"2024-01-06T15:04",updated:"2024-01-06T15:04"},s="Computer Vision",o={id:"AI/Data Science/Computer Vision",title:"Computer Vision",description:"Vision Transformer",source:"@site/docs/AI/Data Science/Computer Vision.md",sourceDirName:"AI/Data Science",slug:"/AI/Data Science/Computer Vision",permalink:"/docs/AI/Data Science/Computer Vision",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI/Data Science/Computer Vision.md",tags:[{label:"article",permalink:"/docs/tags/article"},{label:"digital-garden",permalink:"/docs/tags/digital-garden"}],version:"current",frontMatter:{Finished:null,tags:["article","digital-garden"],created:"2024-01-06T15:04",updated:"2024-01-06T15:04"},sidebar:"gardenSidebar",previous:{title:"Data Science",permalink:"/docs/AI/Data Science/"},next:{title:"Data Science Project Start-Up Phase",permalink:"/docs/AI/Data Science/Data Science Project Start-Up Phase"}},c={},l=[{value:"Vision Transformer",id:"vision-transformer",level:3}];function d(e){const t={a:"a",h1:"h1",h3:"h3",li:"li",p:"p",ul:"ul",...(0,a.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"computer-vision",children:"Computer Vision"}),"\n",(0,i.jsx)(t.h3,{id:"vision-transformer",children:"Vision Transformer"}),"\n",(0,i.jsx)(t.p,{children:"Vision transformer applies the transformer architecture to computer vision algorithms. In ViTs, an image is split into fixed-size patches. These patches, along with positional encodings, are flattened and linearly embedded to retain information about their original position in the image. The transformer then processes these sequences of embeddings.\nIn 2021, a pure transformer model demonstrated better results than Convolutional Neural Networks. However, they require large amounts of data. Interpretability is also better because the attention mechanism in transformers can be inspected to understand which parts of the image are being focused on for a particular decision."}),"\n",(0,i.jsx)(t.h1,{id:"links",children:"Links"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{}),"\n"]}),"\n",(0,i.jsx)(t.h1,{id:"thoughts",children:"Thoughts"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Never use the ",(0,i.jsx)(t.a,{href:"https://pypi.org/project/Pillow/",children:"pillow"})," if you can use ",(0,i.jsx)(t.a,{href:"https://pypi.org/project/opencv-python/",children:"OpenCV"}),". The Pillow package usually mentions a lot of things in the image that might lead to bugs."]}),"\n"]})]})}function u(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},11151:(e,t,n)=>{n.d(t,{Z:()=>o,a:()=>s});var i=n(67294);const a={},r=i.createContext(a);function s(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);